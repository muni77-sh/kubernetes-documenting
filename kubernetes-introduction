Cluster Architecture:
--> kubernetes architecture
--> ETCD 
--> Kube-API server
--> Controller managers
--> kube scheduler
--> kubelet
--> kube proxy


Master and worker nodes
---------------------
1. ETCD:
is a distributed reliable key value store that is simple, secure and fast.

--> Key-value store:
its tabular/Relational Databases

--> Install ETCD:
1. download Binaries:
curl -L https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz -o etcd-v3.3.11-linux-amd64.tar.gz

2. extract 
tar xzvf etcd-v3.3.11-linux-amd64.tar.gz

3. run ETCD service
./etcd

Default port for etcd listen: 2379

to set key-value
./etcdctl set key1 value1
./etcdctl get key1
./etcdctl

---------------------
when you setup--kubeadm
kubectl get pod -n kube-system

to list all the key-run inside the etcd-master POD
--> kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only

ETCD -commands (optional)
ETCDCTL is the CLI tool used to interact with ETCD

ETCDCTL can interact with ETCD server using 2 API versions
-version 2 and version 3.

ETCDCTL version 2 supports:
--> etcdctl backup
--> etcdctl cluster-health
--> etcdctl mk
--> etcdctl mkdir
--> etcdctl set

the commands are different in versions 3:
--> etcdctl snapshot save
--> etcdctl endpoint health
--> etcdctl get
--> etcdctl put

To set the right version of API set the environment variable ETCDCTl_API=3

When API version is not set, it is assumed to be set to version 2 and 3

ETCDCTL can authenticate to the ETCD API server.
The Certificate files are available in the etcd-master at the following path.

--> cacert /etc/kubernetes/pki/etcd/ca.crt
--> cert /etc/kubernetes/pki/etcd/server.crt
--> key /etc/kubernetes/pki/etcd/server.key

the ETCDCTL API versio and path to certificate files.
--> kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 
-------------------------
2. kube-API server:
--> curl -X POST /api/v1/namespaces/default/pods ..
pods created

1. Authenticate user
2. Validate Request
3. Retrieve data
4. Update ETCD
5. Scheduler
6. kubelet

installing kube-api server
--> wget https://storage.gooleapis.com/kubernetes-release/release/v1.13.0/bin/liux/amd64/kube-apiserver
--(kube-apiserver.service)

--> View api-server options - kubeadm
cat /etc/kubernetes/manifests/kube-apiserver.yaml

cat /etc/systemd/system/kube-apiserver.service

running process
ps -aux  | grep kube-apiserver

--------------------------------------------
3. Kube Controller Manager:

--> Node-controller
 -> watch status
 -> remediate situation
 -> node monitor perios = 5s
 -> node montitor grace period = 40s
 -> pod eviction timeout = 5m

--> Replication-Controller

----
Installing kube-controller-manager
--> wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-controller-manager

kube-contrller-manger.service

-> View kube-controller-manager -kubeadm
cat /etc/kubernetes/manifests/kube-controller-manger.yaml

cat /etc/systemd/system/kube-controller-manger.service

ps -aux | grep kube-controller-manager

--> view kube-scheduler options-kubeadm 
cat /etc/kubernetes/manifests/kube-sceduler.yaml

--> to view kube-scheduler 
ps -aux | grep kube-scheduler

--------------------------------------------
4. kubelet
--> register node
--> create PODS
--> Monitor NOde & PODS

--> Installing Kubelet
wget https://storage.googleapis.com/kubernetes-release/v1.13.0/bin/linux/amd64/kubelet

kubelet.service

--> 
ps -aux | grep kubelet

------------------------
kube-proxy
---
to connect pod to pod then kubernets we are going to use

--> Installing kube-proxy
wget https://storage-googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy

kube-proxy.service

--> to view kube-proxy kubeadm
kubectl get pods -n kube-system

kubectl get daemonset -n kube-system
------
Pod
----
kubectl
--> kubectl run nginx --image nginx

--------------------------
pod with yaml

apiVersion: v1
kind: pod
metadata:
  name: nginx
  labels: nginx
    app: nginx
    tier: frontend
spec:
  containers:
    - name: nginx
      image: nginx

--> kubectl create -f pod-defintion.yml
--> detail particular pod
 -> kubectl describe pod myapp-pod

----------------------------
kubectl run nginx --iamge=nginx
kubectl expose pod nginx --port 80 --type nodeport

kubectl run redis --image=redis123 --dry-run=client -o yaml > pod.yaml

apiVersion; v1
kind: pod
metadata:
  creationTimestamp: null 
  labels:
    run: redis
  name: redis
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy; ClusterFirst
  restartPolicy: Always
status: {}
kubectl apply -f pod.yaml

kubectl edit pod redis
------------------------------
ReplicaSets:
---------
--> Replication-controller: helps to multiple instances of single pod
--> high availability

Load Balancing & Scaling:

apiVersion: v1
kind: ReplicationController
metadta:
  name: myapp-rc
  labels:
      app: myapp
      type: front-end
spec:
  template:

  metadata:
   name: myapp-pod
   labels:
      app: myapp
      type: front-end
  spec:
    container:
    - name: nginx-container
      image: nginx
 replicas: 3

--> kubectl create -f rc-defintion.yml 
--> kubectl get replicaset
--> kubectl scale --replicas=6 replicaset-definition.yml
--> kubectl scale --replicas=6 replicaset myapp-replicaset 
--> kubectl delete replicaset myapp-repicaset-->deletes all underlying PODs
--> kubectl repace -f replicaset-definition

---------
Deployments:
------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
      app: myapp
      type: front-end
spec:
  template:
    metadata:
     name: myapp-pod
     labels:
        app: myapp
        type: front-end
     spec:
       containers:
       - name: nginx-container
         image: nginx
     replicas: 3
     selector:
        matchLabels:
           type: front-end
--> kubectl create -f deployment-definition
--> kubectl get deployments
--> kubectl get replicaset
--> kubectl get all

reate an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment

kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.

kubectl create -f nginx-deployment.yaml

OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

-------------------------------
Namespaces:
----------------
to list with names
--> kubectl get pods --namespace=kube-system

apiVersion: v1
kind: pod

metadat:
 name: myapp-pod
 namespace: dev
 labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-container
    image: nginx
--> kubectl create -f pod-defintion.yml
--> kubectl create -f pod-definition.yml --namespace=dev

-----
how you do create new namespace:
apiVersion: v1
kind: Namespace
metadata:
    name: dev
kubectl create -f namespace-dev.yml--> kubectl create namespace dev
--> kubectl get pods --namespace=dev
-->  kubectl get pods
--> kubectl get pods --namespace=prod
--> kubectl config set-context $(kubectl config current-context) --namespace=dev
--> kubcectl get pods --all-namespaces

apiVersion: v1
kind: ResourceQuota
metadata: 
    name: compute-quota
    namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    limits.cpu: "10"
    limits.memory: 10Gi

--> kubectl create -f compute-quota.yaml
--> kubectl get ns --no-headers 
--> kubectl get ns --no-headers | wc -l
--> kubectl -n research get pods --no-headers
--> kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml

apiVersion; v1
kind: pod
metadata:
  creationTimestamp: null 
  labels:
    run: redis
  name: redis
  namespace: finance
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy; ClusterFirst
  restartPolicy: Always
status: {}

--> kubectl apply -f pod.yaml
--> kubectl -n finance get pod redis
--> kubectl get pods --all-namespaces | grep blue
--> kubectl -n dev get svc
--------------------------------------------------------------------------------------------------------------------

Service- NodePort:
----------
apiVersion: v1
kind: service
metadata: 
    name: myapp-service
spec:
    type: NodePort
    ports:
     - targetPort: 80
       *port: 80
        nodePort: 30008
    selector:
       app: myapp
       type: front-end
  
--> kubectl create -f service-definition.yml
--> kubectl get services
--> curl http://192.168.1.2:30008
--> curl http://192.168.1.3:30008
--> curl http://192.168.1.4:30008
-----------------------------------------
Service Cluster IP:
-----------------
apiVersion: v1
kind: Service
metadata:
    name: back-end
spec:
    type: ClusterIP
    ports:
     - targetPort: 80
       port: 80
 
    selector:
       app: myapp
       type: back-end

--> kubectl create -f service-definition.yml
--> kubectl get services
----------------------
Service - Load Balancer
--------------------
apiVersion: v1
kind: Service
metadata:
    name: myapp-service
spec:
    type: LoadBalancer
   ports:
    - targetPort: 80
      port: 80
      nodePort: 30008

--> kubectl get svc
--> kubectl describe svc kubernetes
--> kubectl get deployments.apps
--> kubectl describe deployments.apps simple-web-deployment | grep -i image
--> kubectl expose deployment simple-web-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=clinet -o yaml > svc.yaml

apiVersion: v1
kind: Service
metadata:
  creationTimeamp: null
  name: webapp-service
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    nodePort: 30080
  selector:
    name: simple-webapp
  type: NodePort
status:
  LoadBalancer: {} 
--> kubectl apply -f svc.yaml
--------------------------------------------
Imperative vs Declarative:-
-------------------------------
Infrastructure as code:
------------------
Imperative
--> kubectl run --image=nginx nginx
--> kubectl create deployment --image=nginx nginx
--> kubectl expose deployment nginx --port 80
--> kubectl edit deployment nginx
--> kubectl scale deployment nginx --replicas=5
--> kubectl set image deployment nginx nginx=nginx:1.18
--> kubectl create -f nginx.yaml
--> kubectl replace -f nginx.yaml
--> kubectl delete -f nginx.yaml

----
Declarative
---
--> kubectl apply -f nginx.yaml
----------------------
Imperative Commands:
--
Create objects:
--> kubectl run --image=nginx nginx
--> kubectl create deployment --image=nginx nginx
--> kubectl expose deployment nginx --port 80
--
update objects:

--> kubectl edit deployment nginx
--> kubectl scale deployment nginx --replicas=5
--> kubectl set image deployment nginx nginx=nginx:1.18
-------
Imperative object Configuration Files:
------------------
Create objects:

apiVersion: v1
kind: pod

metadata:
  name: myapp-pod
  labels:
     app: myapp
     type: front-end

spec:
  containers:
  - name: nginx-container
    image: nginx
--> kubectl create -f nginx.yaml
-----
update objects:
----
--> kubectl edit deployment nginx 

apiVersion: v1
kind: pod

metadata:
 name: myapp-pod
 lables:
     app: myapp
     type: front-end
spec:
   containers:
   - name: nginx-container
     iamge: nginx:1.18
status:
  conditions:
  - lastProbeTime: null
    status: "True"
    type: Initialized

------------------------
apiVersion: v1
kind: pod

metadata:
 name: myapp-pod
 labels:
    app: myapp
    type: front-end-service
spec:
  containers:
  - name: nginx-container
    iamge: nginx:1.18

--> kubectl replace -f nginx.yaml
--> kubectl replace --force - nginx.yaml
-----------------------------------
Declarative:
-------------------
create objects
---
apiVersion: v1
kind: pod

metadata:
 name: myapp-pod
 labels:
    app: myapp
    type: front-end-service
spec:
  containers:
  - name: nginx-container
    image: nginx:1.18
--> kubectl aply -f nginx.yaml
--> kubectl apply -f /path/to/config-files

Update objects
--
--> kubectl apply -f nginx.yaml
------------------------------------------
dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , 
use the --dry-run=client option. 
This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
--------------------
-o yaml: This will output the resource definition in YAML format on screen.

POD
Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml

Deployment
Create a deployment

kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml


Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4


You can also scale a deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml

You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
(This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. 
So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. 
You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. 
I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using 
the same command and manually input the nodeport before creating the service.

--> kubectl run nginx-pod --image=nginx:alpine
--> kubectl run redis --image=redis:alpine --labels=tier=db
--> kubectl expose pod redis --name redis-service --port 6379 --target-port 6379
--> kubectl describe svc redis-service
--> kubectl create deployment webapp --image=kodekloud/webapp-color
--> kubectl sclae deployment --replicas=3 webapp
--> kubectl run custom-nginx --image=nginx --port 8080
--> kubectl describe pod custom-nginx 
--> kubectl create ns dev-ns
--> kubectl create deployment redis-deploy --iamge=redis --namespace=dev-ns --dry-run=client -o yaml > redis.yaml

 















































